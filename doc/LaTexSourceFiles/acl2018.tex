\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}

\graphicspath{ {images/} }

\aclfinalcopy

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{A Novel Approach for Generating Dialogue Scripts From GOT With LSTM, RNN, GPT-2 Language Model}

\author{ Sajid Ahmed - 1610364042\\
Arifuzzaman Arman 1610551042\\
Zahin Akram - 1610618042\\
Md. Rakib Imtiaz - 1610294642}
\date{}


\begin{document}
\maketitle

\begin{abstract}

Generating human quality text,especially like popular TV series scripts or books, is a challenging problem because of ambiguity of meaning and difficulty in modeling long term semantic connections. Recurrent Neural Networks (RNNs)\cite{mikolov2010recurrent} have shown promising results in this problem domain,
with the most common approach to its training being to maximize the log predictive likelihood of each true token in the training sequence given the previously observed tokens.But RNNs have a short-term memory which is why Long Short Term Memory (LSTM)\cite{gers1999learning} predicts better keeping semantic connection.On the other hand recently released state of the art transformer based language model, Generative Pre-Trained Transformer version 2(GPT-2), showed interesting results in various fields of NLP,including text generating\cite{radford2019language}.Here in this project we generate texts by training on the Game of Throne TV series script with these three models and compare them to find  how OpenAI GPT-2 is better at generating novel text.

\end{abstract}



\section{Introduction}
Text generation has a lot of uses which includes chatbot, Question \& Answer system, text summarizing and content creation. Most commercially successful text generation systems are the one which can create summaries of data sets. Due to the recent advances made in the field of Natural Language Processing (NLP) like Bidirectional Encoder Representations from Transformers(BERT) Language model\cite{devlin2018bert} text generation has never been this easy to train .And the advancement in  summarizing\cite{salton1994automatic}, In this paper, we use some popular models of recent times to compare their performance for text generation. We use relatively simple models such as Recursive Neural Network (RNN) \cite{mikolov2010recurrent}, Long Short-Term Memory Units (LSTM)\cite{gers1999learning} along with a more complicated language model called GPT-2\cite{radford2019language}. We train all three models using the same data in the same environment for 100 epochs. We then compared the trained models via loss curves ,perplexity distribution and Qualitative judgement.
\section{Models}
One of key components on which deep learning models are built are neural networks.A neural network takes in inputs, which are then processed in hidden layers using weights that are adjusted during training.In the models we compared there were other components along with the neural networks which made them unique and better. This section has in-depth details of the models compared in this paper and what makes them better for text generation.

\subsection{RNN}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{RNN.png}
\caption{Recurrent Neural Network(RNN)}
\end{figure}

RNNs are artificial neural network where the computation graph contains directed cycles \cite{mikolov2010recurrent}. In RNN, information travels in loop from layer to layer unlike feed forward neural network where the information travels strictly in one direction from layer to layer. The output of a layer in RNN depends on the input and the output of the previous state. This allows the model to store memory about it's past computation. If at time t, the input is \(x_t\) and the output of previous layer \(h_{t-1}\), then the output of the current layer \(h_t\) is,
\begin{equation}
   \ h_t = f(x_t, h_{t-1})
\end{equation}
RNN exhibit dynamic temporal behavior and can model temporal sequences of input-output pairs. That makes it a good model to use for text generation.

\subsection{LSTM}
LSTM is a special type of RNN. Simple RNN has a shortcoming. When training simple RNN via back-propagation, back-propagated gradients tend to zero (Vanishing gradient problem) or to infinity (Exploding gradient problem) due to the computation\cite{gers1999learning}. LSTM partially addresses the issue by allowing the gradient flow to remain unchanged which solves the vanishing gradient problem. However LSTM still suffers from exploding gradient problem.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{LSTM.png}
\caption{Long Short-Term Memory (LSTM)}
\end{figure}

A single LSTM unit is made of a cell and three gates. These gates control the information entering and exiting a cell. The input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. The gates filters the information based on their own sets of weights. Those weights like the weights of input and hidden layers are adjusted through training. Using the gates the model can be trained so that only relevant information gets passed on and irrelevant information is forgotten.

The equations for the gates in LSTM are:
\[i_t= \sigma(w_i[h_{t-1},x_t]+b_i) \]
\[f_t= \sigma(w_f[h_{t-1},x_t]+b_f) \]
\[o_t= \sigma(w_o[h_{t-1},x_t]+b_o) \]
Here \(i_t\), \(f_t\), \(o_t\) refers to the outputs of input gate, forget gate and output gate at timestamp \(t\). \(w_i\), \(w_f\), \(w_o\) are the weights of input gate, forget gate and output gate. \(b_i\), \(b_f\), \(b_o\) are the biases of input gate, forget gate and output gate. \(\sigma\)  represents sigmoid function. \(h_{t-1}\) is the output of the previous LSTM unit and \(x_t\) is the input of the current LSTM unit.

So, we can use LSTM for text generation on larger data-sets to get better results.

\subsection{OpenAI GPT-2}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth,height=8cm]{GPT-2.png}
\caption{Structure of GPT-2 Model}   
\end{center}
\end{figure*}

Generative Pre-Trained Transformer version 2(GPT-2) is a large transformer based language model made by OpenAI\cite{radford2019language}. A transformer is an encoder decoder based architecture with an attention mechanism. The model first finds the embedding vectors for the input sequence and adds positional embedding with it. The embedding are then taken as input into an attention layer.The attention layer finds the self attention of all input embedding vectors. 

Here ,the dimension of queries and keys is \(d_k\). We compute the dot products of the query with all keys, divide each by \sqrt{\(d_k\)}, and apply a softmax function. We multiply the output of softmax function with value to get the score for self attention. In practice, all queries are packed together in a matrix \(Q\). Similarly all keys and values are packed together in \(K\) and \(V\) matrices. \(Attention(Q,K,V)\) is the matrix of outputs. We calculate it using,

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

The self attention has residual connection with the inputs of the attention layer. The output is then put through a feed forward network. The decoder is of similar design to the encoder. However, to ensure that the model does not look cheat, all output values after the current one is masked. The values of a masked embedding vector are near negative infinity, so their softmax value tends to zero. After the output goes through a masked self attention layer, they are put through another self attention layer where weights from the encoder's self attention is used. The output goes through a feed forward network and a softmax function to make the next prediction.

GPT-2 has 12 layers with each layer having 12 attention heads. So the model can find different types of connections between all the words. This makes it very suitable for text generation.
\section{Datasets and Experiments}
In this section, we discuss our data set, the parameters of our models and the evaluation metrics used to compare the models.

\subsection{Dataset}
We ran all our experiment on a data set about the series game of thrones which we collected from the site  \href{https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=game-of-thrones&episode=s01e01}{Springfield}. Our corpus contains a large collection of fictional conversation collected from the script of the series. The data set has  unique characters and unique tokens.

We needed an example of data that was significant in size and varying in qualities for different characters. Out of all available data, the data about Game of Thrones met all our criteria.

   
\subsection{Experiment}
For LSTM and RNN we divided our data set into two parts: 80\% of the data set is used for training and  20\% of the data set is used for testing.For GPT-2 we divided the dataset into 10\% validation set and 90\% training set.We trained three different models: RNN, LSTM and GPT-2 which we have explained in a previous section of this paper.
We used embedding length 1024 and a single hidden layer for RNN. We used 1024 embedding length and a single hidden layers for LSTM.We used batch size 124 in both RNN and LSTM. The implementation of the GPT-2 is obtained from the official git \href{https://github.com/openai/gpt-2}{repository}. GPT-2 has multiple variations smaller model(117M),medium(345M) and a larger model which was not released. We used the medium version called 345M. GPT-2 model had twelve layers with each layer twelve headed attentions.We fine tuned the GPT-2 model and trained on the GOT data-set for one hundred epochs. We saved all the weights after each twenty five epochs so that we can check the progress of the models. 
The configuration of the system used for training our models is:
\begin{enumerate}
    \item CPU: Intel 7th Gen Core-i7 7700K @4.00GHz 4 Cores 8 Threads CPU
    \item RAM: 16 GB DDR4 @2400MHz 
    \item GPU: NVIDIA GTX 1070 @8GB GDDR5 VRAM Founders Edition
    \item GPU 2(In Google Colab): Tesla K80 @ 24GB GDRR5 VRAM 
    
\end{enumerate}

\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\linewidth]{RNNLoss.png}
\end{center}
\caption{Loss Curve of RNN}
\end{figure}
   
\subsection{Evaluation Metrics}
Since we want to compare the models, we need evaluation metrics to compare them by. We evaluate the models on loss, perplexity and example sentences for qualitative judgements.

\begin{enumerate}
  \item Loss: We have calculated cross-entropy loss for both training and test sets.\cite{zhang2018generalized}.
  \item Perplexity: Perplexity is a measurement of how well a probability distribution predicts a sample. It is usually calculated as \(2^H\), where \(H\) is the cross-entropy loss\cite{horgan1995complexity}.
  \item Qualitative judgment: Since the goal of text generation is to generate realistic human-like texts, we have decided to generate samples from all three models and evaluate them from a human's perspective.\cite{bengio2015scheduled}.
\end{enumerate}

\section{Results}

Results are the most important part of any evaluation. For the results we generated loss curve graph and perplexity graph in \href{https://www.tensorflow.org/guide/summaries_and_tensorboard}{Tensorboard}, using the checkpoint files created by training the GPT-2 model. Below we provide our methods of evaluation and visual representations necessary for evaluation.
\newline

\begin{tabular}{ |p{2 cm}||p{1cm}|p{1.5cm}| }
 \hline
 \multicolumn{3}{|c|}{\textbf{Result Table}} \\
 \hline
    Model Name & Loss & Perplexity\\
 \hline
     RNN   &  0.97   &  24.08 \\
     LSTM  &  0.63  &   28.12 \\
     GPT-2 &  0.06   &  44.10  \\
 \hline
 
\end{tabular}


\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\linewidth]{RNNPerp.png}
\end{center}
\caption{Perplexity Graph RNN}
\end{figure}


\section{Output Results}
 
\subsection{RNN Sample Outputs}

\textbf{Epoch 25:}:\newline 

Jaime Lannister what she won’t be a Brother looks in I do.
Tamor Clegane: It's a drink.
JON: That had me. He has three dragons are scream until your chair days, softer than fly to play with the throne sleep in the clothes. 
Daenerys dissusts him across the childo Laughants.
GUARD: A bit chips as she is warning the page of your neck. Lost Watches of mourning it taking the Pyp return us the only princess, from that?
All the other man is seated by one but you take their wayste at she can off his dagger will be heard rush down a letter with your direction filled in that black from her price.

BRAN: Because they ordered Theon. And every man to deach - hain cut the room and takes to the third, of course, Your Grace?

JAIME: Princess grinches follow wine was talking to your nistance. DAVOS and HARDAYR looks around. And he will be pight them off with me, looking one shall would I saw the Boltons who brantly.
Jon Snow: I was born to kiss Lakilliod throws some ships and bler ends and locks\newline 

\textbf{Epoch 50:}:\newline 

Jaime Lannister trance in the throne. I can take each other and revealing ARYA go to the fraulttard? Do you have to be an ourt watching his panted] I'll graze me once. He stops and looks back as entering her blood tell me, then. Falling himself. Edd as the Dothraki encampment.

YOUNG LYANNA's land. He leads up his hand on the watch and I will not. What are it for their kills we want. A man in the Conquering on Tommen, who wants to use think it weren'tly cliffs.

DANY: Who conquer! Toyages too right? Ohowaces sawards.
Jorah Mormont: King Joffrey, Uncling one. Stop it days, Your Grace.

PROSTITUREE walks to his chins, then looks at JON and his blood of my lady.On lord.

MEERA is disconds loudly loom around a glass). I took it will for this difference? Next to sing on the oath.

JON SNOW: Old and see a small begined cities. We calls a gold Commander. It didn't expet. WUN WUN is her cup out old man.

MAN #2: Her Khal after him. Not growledgunted him.

DAARIO: Because that was praidon.\newline 

\textbf{Epoch 60:}\newline 

Jaime Lannister always. That is not a raven from castles at. The trail of down their throats a Lannister. Myn last nothing something didn't put it.
Daenerys DAENERYS: You primen larger their cunt Luwin: Why don't you destroyed to him a little lord.
Jon Snow: Go.
Daerae drunk. Wearing it larger. (nods his horse remains. The scal members of the Faith Militantic.
He has a face, he will have you blind for thousands of warm / Fasless out,. You, we’re words. We can have means to me.

GENDRY: “Whipsters for his back to VARY

Arya puts a hand on the hull.

OLENNA: As he leans you, but a twh, but junting open carrying all over the King has a mimmoments. She hears forward). Tyrion Lannister: I had tradition you.

TYRION: What are they should join diving or affless and blood swings, did these Seven Dearl wak was the will. Did Lord Bromies. We truly: Lord Tyrell.

STANNIS: Get out.

JORAH: Knugh.

EXT. BRAAVOS - MARIAN’s sled.

RAMSAY: I didn’t know: We should have been Baratheon, is would breath\newline 

\subsection{LSTM Sample Outputs}

\textbf{EPOCH 25:}\newline 

Tyrion lannister: Oh, is she? Oh.
Petyr Baelish: What's your actual felt of that clean done, it's what we different and engulfed in black as Hand.

CERSEI walks in. She pulls a gase cloak as everyone. I’m afraid I don't really walk and wniten imprison you can die in Hodor.

HOTHAR: (Valyrian) “have. I meant the d.

SANSA and BAELISH hand in to see if it’s command to kill them; Ker banner. You're our wound, hat's all right. It's mad, l admursing things.

SANSA: And it was mean of called King Robin is really not because everyone know manbers from the Lord's Company. Poor boy.

SAM: Jon's that.

SAM: The Starks and the Bastard of Winterfell, now and father cut through the right to take my men and rush into the water. It is the Lord's wife.

HOUND: Well, that's the have another child, a woman let towards Jaime and set of money oo Lord of the Seven Kingdoms. Bring him against scanting. He heards take it back to SAM.

GENDRY: You don't understand why you can love it? Look at me. You bease, pu\newline 

\textbf{EPOCH 50:}\newline 

Tyrion lannister: So why should it was be.
Robin Arryn: Is there gone to Varys? Kevang Lord Baelish sounds like quite in victory and give my mind! Promise me, Khaleesi. I do not a child.

TYRION: It could be, but l you mention of Stannis must at war. We know where is still sat there is, we don't remember. Benjone, it's true. You were terious, stayed on a spike. It won't say you know what I am, while, isn't it? A woman will never be able to my house.

Jon shoots to get his sword; Theon appears burning. Astaporsarryes, as soon as Tommen of the North.

JON: Lord Commander Mormont lis making live the time.
Stannis Baratheon: But, is Lord Stark, a moment.
Aponeck as Valyrian ster: Cheot for a King.
Greatjon Umber: One army, a hand on me. Listening to me, I will have no one. l've seen more house, but I'm a knight. No, you're not saying before I embaround and hundreds of glass of wine.

TORMUND: The night is dark and full of terrors in distrust covorrik: You will not here quick. Deerys. One day, my\newline 

\textbf{EPOCH 75:}\newline 

Tyrion lannister: There will be put your honor is that Greyscale.

RANDYLL: Oh, do you have any good wine for proven how did it end unarmed, turns his horse's Bay. I didn’t fight for you. Because you don't have an attention or a truber: You are no ranging man. We enjoy she was two kings of Mereen's body]
Petyr Baelish: (singing) She did.
He named him.
Qut this time you have, the wrong king honor. We have a sparrow, they break into a fighter?
Davos Seaworth: I thought you were gone for your baby. You'll absold for a Volunt's son.

STANNIS: Grent over. JAIME is pleading to at each other’s arms. Jaime looks away.

\begin{figure}[h]
\includegraphics[width=1.0\linewidth]{LSTMLoss.png}
\caption{Loss curve of LSTM}
\end{figure}

GREN: You jem! l don't want to know if there's a queen because of your legse? That their sporves in a pile of breaks out from her and patched me this south in livings, but takes one of the hand that, as they and Jeor Mormont rise, causing, one of these nights!

Jon starts e it?

ARYA: You're an idiot. Feed, givenin in chair. It spiness is the most power up a certaintown.

CERSEI:whatever !!\newline 


\textbf{Epoch 100:}\newline 

Tyrion lannister: Every love commands anymore. Terriblack my head against us.

Tyrion opens the door, then begins carving so Loboda still man extends through the linestand pinements of all possibley successful.

DAVOS and GENDRY stand on the plate. MELISANDRE picks it up. The Night’s Watch. He is there betwer the question?

FENNESZ: Indeed I was with you.

ARYA pulls away. BRAN walks into ite are carrying two reachike) He wouldn't be here before anyone else.

JON: And how to give you a scar?

TYRION: Excellent child, she won't be needing a smart for a child. If we beside them, to the swords through, he leaded. How: He can't be.
Robb Stark: I was looking for a glass of wildfire-able that does still alive? Everything I told you it's a lie to her face, the Night's Watch! Stop sail, and now the Lannisters.
Rickard Karstark: I thought you told me abandon the stairs.
Maester Luwin: Well done.
Add says nothing.

STANNIS: Are you sure you undo to wait?

SANSA: He's still the survival screech

\begin{figure}[h]
\includegraphics[width=1.0\linewidth]{LSTMPerp.png}
\caption{Perplexity Graph LSTM}
\end{figure}


\subsection{GPT-2 Sample Outputs}

GPT 2 Sample Generated Text:\newline 

\textbf{Epoch 25:}\newline 

TOMMEN: You're right.\newline  
MEN: I don't know. Something that can help a man like me.\newline 
TOMMEN: I don't think we'd be here if someone wanted us to get along.\newline 
BRIENNE: No, it's all right. I'm not your enemy. That's the trouble that made it difficult for you to like you.\newline  
TOMMEN: I know.\newline 


\textbf{Epoch 50:}\newline 

SANSA: I'm sorry, Father. I didn't mean\newline 
CUT TO: HOUSE OF BLACK AND WHITE - MELISANDRE'S TENT\newline 
MELISANDRE is sitting at a campfire, holding the candles. She wears the same outfit every day.\newline 
CUT TO: INT. HOUSE OF BLACK AND WHITE – TENT\newline 
MELISANDRE is lying in fetal of a blanket. The fire is burning. She is having trouble breathing. She finally falls asleep. Suddenly, a sound can be heard from outside.\newline 

VARYS and TYRION are walking together across the harbor. TYRION begins inspecting the map of the city together.\newline 
TYRION: Interesting things. This one looks like a plan. This one here.\newline
VARYS: I like the color, eh? Dark olive olive Dark chocolate\newline 
TYRION: If I wanted someone to paint the face, I'd hire an assistant. I'd hire an assassin.\newline 

\begin{figure}[h]
\includegraphics[width=1.0\linewidth,height=4cm]{GPTLoss.PNG}
\caption{Loss Curve of GPT-2}
\end{figure}

VARYS: How disappointing for you.\newline 
TYRION: Every day, I'm told I am. This one isn't working for me.\newline 
VARYS: You don't think they will.\newline 


\textbf{Epoch 75:}\newline 

INT. KING’S LANDING - ROOM\newline 
CERSEI: You promised me you could kill yourself. You said it would be suicide.\newline 
KEVAN: Lies are our only chance.\newline 
CERSEI: Get out.\newline 
KEVAN: Your place is in the capital with my son.\newline 
CERSEI: I’m the king, not a stinking corpse. I’m the queen.\newline 
KEVAN: No, but if this is the last thing you do before you die, I give them the satisfaction of thinking you are the queen.\newline 
CERSEI: I will kill him myself.\newline 

\textbf{Epoch 100:}\newline 

DAVOS and TORMUND are walking along the campsite.\newline 
TORMUND: If you were Jon we were this fight would be here already.\newline 
DAVOS: As long as we’re on one side, we have a chance. We have to get the size of the Giant. We have to hit him from both sides. If they got the numbers, the large scale will be outnumbered.\newline 
TORMUND: How many brothers do we have in the army of the Dead?\newline 
DAVOS: We can find out. More than enough.\newline 
DAVOS and TORMUND proceed out to Mo burning log. They begin walking along a path built into the mountain. They happen upon a ring of eight other Wildlings which DAVOS is following.\newline 


\begin{figure}[h]
\includegraphics[width=1.1\linewidth,height=4cm]{GPTPerp.png}
\caption{Perplexity Graph GPT-2}
\end{figure}


\section{Conclusion}
From the evaluation we can clearly see how GPT-2 is better at generating text from the Game of Thrones Script. Also, the LSTM showed far better results than RNN.However, LSTM took a much longer time to train.So the trade off may not be worth it. 

However in GPT-2 ,with noise, it took about 8-9 hours to train.To make it better we trained around 600000 iterations which took, for our GPU, 4 days. However, significant improvements in generation serves to show that the time spent in refining the model was worth it. In future we plan on to build or modify a language model using transformer for text generation.

For ages people are writing and communicating through language. Through language we  started telling fictional stories which inspire us and teach us important lessons in life. We  wanted to find the best artificial story teller,like GPT-2, which can inspire us and help us create better models in future which can tell even better stories.  

\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\appendix
\end{document}
